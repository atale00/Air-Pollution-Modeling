---
title: "Untitled"
author: "Xinyi Liu"
date: "2017年12月14日"
output: html_document
---

```{r,echo=T}
options(warn=-1)
library(readr)
library(dummies)
library(corrplot)
library(plyr)
library(ggplot2)
library(leaps)
library(caret)
library(mlbench)
library(randomForest)
library(xgboost)
library(glmnet)
library(boot)

```
1. Import data
```{r,echo=TRUE}
library(readxl)
data2014 <- read_excel("E:/实习/2014.xlsx")
library(dplyr)
library(tidyr)
new14 = data2014 %>%
   separate(X__1, c("date", "time"), " ")
#sheet2 <- read_excel("E:/实习/new2014.xlsx", 
#    sheet = "2")
#data2014=cbind(sheet1new,sheet2[,-1])

newdata <- na.omit(new14)
summary(newdata)
sapply(newdata,class)
data2=newdata[,-1][,-1][,-4][,-4][,-4]


```
data2 above contains all original data after omitting NA values. Now we want to subset the original datasets into 4 smaller datasets sorting by different wind directions.
```{r,echo=T}
attach(data2)
north=subset(data2,(WD>=0 & WD <=45)|(WD>315 & WD<=360))
east=subset(data2,WD>45 & WD<= 135)
south=subset(data2, WD>135 & WD<=225)
west=subset(data2, WD>225 & WD<=315)

north$direct='north'
east$direct='east'
south$direct='south'
west$direct='west'
north$nt=1
north$es=0
north$st=0
north$ws=0
east$nt=0
east$es=1
east$st=0
east$ws=0
south$nt=0
south$es=0
south$st=1
south$ws=0
west$nt=0
west$es=0
west$st=0
west$ws=1


north=north[,1:7]
east=east[,1:7]
south=south[,1:7]
west=west[,1:7]
appdata=rbind(north,east,south,west)
```




2. Explorary Data Analysis
```{r,echo=TRUE}
par(mfrow=c(1,1))
correlationMatrix <- cor(data2)
corrplot(correlationMatrix, method = "circle")
round(correlationMatrix,2)

corrN <- cor(north[,1:7])
corrplot(corrN, method = "circle")
round(corrN,2)

corrE <- cor(east[,1:7])
corrplot(corrE, method = "circle")
round(corrE,2)

corrS <- cor(south[,1:7])
corrplot(corrS, method = "circle")
round(corrS,2)

corrW <- cor(west[,1:7])
corrplot(corrW, method = "circle")
round(corrW,2)

detach(data2)
attach(appdata)

```
From the correlation plot, we can have the first glance at the data, and we can see that SO2, NO2, CO, O3, Temp, RH and Wind Direction are correlated with PM2.5. However, we should also notice that Temp and Pressure, Temp and RH tend to be strongly correlated.

From 4 plots generated by separate subsets above, we can figure out that north wind and east wind does not affect PM2.5 significantly, while south wind and west wind may be strongly correlated with PM2.5.

Since we detect some collinearity in this dataset, we may consider different methods to deal with it, such as: stepwise regression, Lasso Regression, random forest, PCA first then xgboost.

3. Model Selection

a. Forward Selection
i. all data
```{r,echo=T}
regfit.fwd=regsubsets(PM2.5~.,data=appdata,nvmax=6,method="forward")
regsum=summary(regfit.fwd)

# plot rss
library(ggvis)
rsq <- as.data.frame(regsum$rsq)
names(rsq) <- "R2"
rsq %>% 
        ggvis(x=~ c(1:nrow(rsq)), y=~R2 ) %>%
        layer_points(fill = ~ R2 ) %>%
        add_axis("y", title = "R2") %>% 
        add_axis("x", title = "Number of variables")

# From this plot, it's better to select 5 or 6 features.


#cross-validation
library(ISLR)
library(knitr)
predict.regsubsets=function(object,newdata,id,...){
	form=as.formula(object$call[[2]])
	mat=model.matrix(form,newdata)
	coefi=coef(object,id=id)
	xvars=names(coefi)
	mat[,xvars]%*%coefi
}
k=10
set.seed(1)
folds=sample(1:k,nrow(appdata),replace=TRUE)
cv.errors=matrix(NA,k,6,dimnames=list(NULL,paste(1:6)))

for(j in 1:k){
	best.fit=regsubsets(PM2.5~.,data=appdata[folds!=j,],nvmax=6,method='forward')
	for(i in 1:6){
		pred=predict.regsubsets(best.fit,appdata[folds==j,],id=i)
		cv.errors[j,i]=mean((appdata$PM2.5[folds==j]-pred)^2)
	}
}

mean.cv.errors=apply(cv.errors,2,mean)
which.min(mean.cv.errors)

#We see that cross-validation selects an 5-variable model.

regfit.fwd=regsubsets(PM2.5~.,data=appdata[,1:7],nvmax=5,method="forward")
regsum=summary(regfit.fwd)
fwdco=coef(regfit.fwd,5)
sort(abs(fwdco), decreasing=T)
```
Forward subset selection selected 5 top variables for us. They are Temp, RH, SO2, NO2, WD.

ii. North data
```{r,echo=T}
regfit.fwd=regsubsets(PM2.5~.,data=north,nvmax=6,method="forward")
regsum=summary(regfit.fwd)

# plot rss
library(ggvis)
rsq <- as.data.frame(regsum$rsq)
names(rsq) <- "R2"
rsq %>% 
        ggvis(x=~ c(1:nrow(rsq)), y=~R2 ) %>%
        layer_points(fill = ~ R2 ) %>%
        add_axis("y", title = "R2") %>% 
        add_axis("x", title = "Number of variables")

# From this plot, it's better to select 5 or 6 features.
regfit.fwd=regsubsets(PM2.5~.,data=north,nvmax=6,method="forward")
coef(regfit.fwd,6)


#cross-validation
predict.regsubsets=function(object,newdata,id,...){
	form=as.formula(object$call[[2]])
	mat=model.matrix(form,newdata)
	coefi=coef(object,id=id)
	xvars=names(coefi)
	mat[,xvars]%*%coefi
}
k=10
set.seed(1)
folds=sample(1:k,nrow(north),replace=TRUE)
cv.errors=matrix(NA,k,6,dimnames=list(NULL,paste(1:6)))

for(j in 1:k){
	best.fit=regsubsets(PM2.5~.,data=north[folds!=j,],nvmax=6,method='forward')
	for(i in 1:6){
		pred=predict.regsubsets(best.fit,north[folds==j,],id=i)
		cv.errors[j,i]=mean((north$PM2.5[folds==j]-pred)^2)
	}
}

mean.cv.errors=apply(cv.errors,2,mean)
which.min(mean.cv.errors)

#We see that cross-validation selects an 5-variable model.

regfit.fwd=regsubsets(PM2.5~.,data=north,nvmax=5,method="forward")
regsum=summary(regfit.fwd)
fwdco=coef(regfit.fwd,5)
sort(abs(fwdco), decreasing=T)
```
Forward subset selection selected 5 top variables for us. They are WS, Temp, RH, SO2, NO2.

iii. East data
```{r,echo=T}
regfit.fwd=regsubsets(PM2.5~. ,data=east,nvmax=5,method="forward")
fwdco=coef(regfit.fwd,5)
sort(abs(fwdco), decreasing=T)
```
Forward subset selection selected 5 top variables for us. They are WS, Temp, RH, SO2, NO2.

iv. South data
```{r,echo=T}
regfit.fwd=regsubsets(PM2.5~. ,data=south,nvmax=5,method="forward")
fwdco=coef(regfit.fwd,5)
sort(abs(fwdco), decreasing=T)
```
Forward subset selection selected 5 top variables for us. They are WS,RH,SO2,NO2,WD.

v. West data
```{r,echo=T}
regfit.fwd=regsubsets(PM2.5~. ,data=west,nvmax=5,method="forward")
fwdco=coef(regfit.fwd,5)
sort(abs(fwdco), decreasing=T)
```
Forward subset selection selected 5 top variables for us. They are Temp, RH, NO2, WD, SO2.

b. Backward Selection
i. all data
```{r,echo=T}
regfit.bwd=regsubsets(PM2.5~.,data=appdata,nvmax=6,method="backward")
regsum=summary(regfit.bwd)

# plot rss
library(ggvis)
rsq <- as.data.frame(regsum$rsq)
names(rsq) <- "R2"
rsq %>% 
        ggvis(x=~ c(1:nrow(rsq)), y=~R2 ) %>%
        layer_points(fill = ~ R2 ) %>%
        add_axis("y", title = "R2") %>% 
        add_axis("x", title = "Number of variables")

# From this plot, it's better to select 5 or 6 features.


#cross-validation
k=10
set.seed(1)
folds=sample(1:k,nrow(appdata),replace=TRUE)
cv.errors=matrix(NA,k,6,dimnames=list(NULL,paste(1:6)))

for(j in 1:k){
	best.fit=regsubsets(PM2.5~.,data=appdata[folds!=j,],nvmax=6,method='backward')
	for(i in 1:6){
		pred=predict.regsubsets(best.fit,appdata[folds==j,],id=i)
		cv.errors[j,i]=mean((appdata$PM2.5[folds==j]-pred)^2)
	}
}

mean.cv.errors=apply(cv.errors,2,mean)
which.min(mean.cv.errors)

#We see that cross-validation selects an 5-variable model.

regfit.bwd=regsubsets(PM2.5~.,data=appdata,nvmax=5,method="backward")
regsum=summary(regfit.fwd)
bwdco=coef(regfit.bwd,5)
sort(abs(bwdco), decreasing=T)
```
Backward subset selection selected 5 top variables for us. They are Temp, RH, SO2, NO2, WD.

ii. North data
```{r,echo=T}
regfit.bwd=regsubsets(PM2.5~. ,data=north,nvmax=5,method="backward")
bwdco=coef(regfit.bwd,5)
sort(abs(bwdco), decreasing=T)
```
Backward subset selection selected 5 top variables for us. They are WS, Temp, RH, SO2, NO2.

iii. East data
```{r,echo=T}
regfit.bwd=regsubsets(PM2.5~. ,data=east,nvmax=5,method="backward")
bwdco=coef(regfit.bwd,5)
sort(abs(bwdco), decreasing=T)
```
Backward subset selection selected 5 top variables for us. They are WS, Temp, RH, SO2, NO2.

iv. South data
```{r,echo=T}
regfit.bwd=regsubsets(PM2.5~. ,data=south,nvmax=5,method="backward")
bwdco=coef(regfit.bwd,5)
sort(abs(bwdco), decreasing=T)
```
Backward subset selection selected 5 top variables for us. They are WS, RH, SO2, NO2, WD.

v. West data
```{r,echo=T}
regfit.bwd=regsubsets(PM2.5~. ,data=west,nvmax=5,method="backward")
bwdco=coef(regfit.bwd,5)
sort(abs(bwdco), decreasing=T)
```
Backward subset selection selected 5 top variables for us. They are Temp, RH, NO2, WD, SO2.

c. Random Forest Modeling
```{r}
library(readr)
library(dummies)
library(corrplot)
library(plyr)
library(ggplot2)
library(leaps)
library(caret)
library(mlbench)
library(randomForest)
library(xgboost)
library(glmnet)
library(boot)
app1=appdata
spiltdata<-app1[sample(nrow(app1)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(app1)),breaks=10,labels=FALSE)

#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- spiltdata[testIndexes, ]
    trainData <- spiltdata[-testIndexes, ]
    #Use the test and train data partitions however you desire...
     fit <- randomForest(PM2.5 ~ .,
                      data=trainData)
  prediction <- predict(fit, newdata=testData)
  error=mean((prediction-testData$PM2.5)^2)
  if (i==10){
    i=importance(fit)
    varImpPlot(fit)
  }
}
error


# north
n1=north
spiltdata<-n1[sample(nrow(n1)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(n1)),breaks=10,labels=FALSE)

#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- spiltdata[testIndexes, ]
    trainData <- spiltdata[-testIndexes, ]
    #Use the test and train data partitions however you desire...
     fit <- randomForest(PM2.5 ~ .,
                      data=trainData)
  prediction <- predict(fit, newdata=testData)
  error=mean((prediction-testData$PM2.5)^2)
  if (i==10){
    i=importance(fit)
    varImpPlot(fit)
  }
}
error

# east
e1=east
spiltdata<-e1[sample(nrow(e1)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(e1)),breaks=10,labels=FALSE)

#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- spiltdata[testIndexes, ]
    trainData <- spiltdata[-testIndexes, ]
    #Use the test and train data partitions however you desire...
     fit <- randomForest(PM2.5 ~ .,
                      data=trainData)
  prediction <- predict(fit, newdata=testData)
  error=mean((prediction-testData$PM2.5)^2)
  if (i==10){
    i=importance(fit)
    varImpPlot(fit)
  }
}
error

#south
s1=south
spiltdata<-s1[sample(nrow(s1)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(s1)),breaks=10,labels=FALSE)

#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- spiltdata[testIndexes, ]
    trainData <- spiltdata[-testIndexes, ]
    #Use the test and train data partitions however you desire...
     fit <- randomForest(PM2.5 ~ .,
                      data=trainData)
  prediction <- predict(fit, newdata=testData)
  error=mean((prediction-testData$PM2.5)^2)
  if (i==10){
    i=importance(fit)
    varImpPlot(fit)
  }
}
error

#west
w1=west
spiltdata<-w1[sample(nrow(w1)),]

#Create 10 equally size folds
folds <- cut(seq(1,nrow(w1)),breaks=10,labels=FALSE)

#Perform 10 fold cross validation
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- spiltdata[testIndexes, ]
    trainData <- spiltdata[-testIndexes, ]
    #Use the test and train data partitions however you desire...
     fit <- randomForest(PM2.5 ~ .,
                      data=trainData)
  prediction <- predict(fit, newdata=testData)
  error=mean((prediction-testData$PM2.5)^2)
  if (i==10){
    i=importance(fit)
    varImpPlot(fit)
  }
}
error

```
So the test set MSE for the random forest is around 900, with its square root around 30, meaning that this model gives predictions that are within around 30 of the true median value.
From the importance plot, we can see that RH causes the most important influence, much higher than other variables. Then WD, SO2, TEMP, NO2 also contribute significantly. 

# NORTH
So the test set MSE for the random forest is around 600, with its square root around 25, meaning that this model gives predictions that are within around 25 of the true median value.
From the importance plot, we can see that RH causes the most important influence, much higher than other variables. Then SO2, NO2, WS also contribute significantly.  

# EAST
So the test set MSE for the random forest is around 1300, with its square root around 36, meaning that this model gives predictions that are within around 36 of the true median value.
From the importance plot, we can see that RH causes the most important influence, much higher than other variables. Then SO2, NO2, temp also contribute significantly.  

# SOUTH
So the test set MSE for the random forest is around 1000, with its square root around 31, meaning that this model gives predictions that are within around 31 of the true median value.
From the importance plot, we can see that RH causes the most important influence, much higher than other variables. Then NO2,Temp,SO2 also contribute significantly. 

# WEST
So the test set MSE for the random forest is around 1000, with its square root around 31, meaning that this model gives predictions that are within around 31 of the true median value.
From the importance plot, we can see that NO2, Temp, RH causes the most important influence, much higher than other variables. Then SO2 also contribute significantly.

d. Lasso Regression
```{r,echo=T}
x=as.matrix(app1[, 2:7])
cv.lasso <- cv.glmnet(x, y=as.matrix(app1$PM2.5), family='gaussian', alpha=1, standardize=TRUE, type.measure='mse')
bestlam=cv.lasso$lambda.min
lasso.mod=glmnet(x, y=as.matrix(app1$PM2.5),alpha=1,lambda=0.01,family='gaussian',standardize=TRUE)
lasso.coef=coef(lasso.mod)[,1]
lasso.coef[lasso.coef!=0]
sort(abs(lasso.coef), decreasing=T)

#north
x=as.matrix(north[, 2:7])
cv.lasso <- cv.glmnet(x, y=as.matrix(north$PM2.5), family='gaussian', alpha=1, standardize=TRUE, type.measure='mse')
bestlam=cv.lasso$lambda.min
lasso.mod=glmnet(x, y=as.matrix(north$PM2.5),alpha=1,lambda=0.01,family='gaussian',standardize=TRUE)
lasso.coef=coef(lasso.mod)[,1]
lasso.coef[lasso.coef!=0]
sort(abs(lasso.coef), decreasing=T)

#east
x=as.matrix(east[, 2:7])
cv.lasso <- cv.glmnet(x, y=as.matrix(east$PM2.5), family='gaussian', alpha=1, standardize=TRUE, type.measure='mse')
bestlam=cv.lasso$lambda.min
lasso.mod=glmnet(x, y=as.matrix(east$PM2.5),alpha=1,lambda=0.01,family='gaussian',standardize=TRUE)
lasso.coef=coef(lasso.mod)[,1]
lasso.coef[lasso.coef!=0]
sort(abs(lasso.coef), decreasing=T)

#south
x=as.matrix(south[, 2:7])
cv.lasso <- cv.glmnet(x, y=as.matrix(south$PM2.5), family='gaussian', alpha=1, standardize=TRUE, type.measure='mse')
bestlam=cv.lasso$lambda.min
lasso.mod=glmnet(x, y=as.matrix(south$PM2.5),alpha=1,lambda=0.01,family='gaussian',standardize=TRUE)
lasso.coef=coef(lasso.mod)[,1]
lasso.coef[lasso.coef!=0]
sort(abs(lasso.coef), decreasing=T)

#west
x=as.matrix(west[, 2:7])
cv.lasso <- cv.glmnet(x, y=as.matrix(west$PM2.5), family='gaussian', alpha=1, standardize=TRUE, type.measure='mse')
bestlam=cv.lasso$lambda.min
lasso.mod=glmnet(x, y=as.matrix(west$PM2.5),alpha=1,lambda=0.01,family='gaussian',standardize=TRUE)
lasso.coef=coef(lasso.mod)[,1]
lasso.coef[lasso.coef!=0]
sort(abs(lasso.coef), decreasing=T)
```
#ALL
Temp, RH, WS, SO2, NO2, WD

#NORTH
WS,Temp,RH,SO2,NO2,WD

#EAST
WS,Temp,RH,SO2,NO2,WD

#SOUTH
WS,RH,SO2,NO2,Temp, WD

#WEST
Temp,RH,WS,NO2,WD,SO2

e. Boosting
```{r}
# PCA
#To examine variability of all numeric variables
sapply(app1,var)
range(sapply(app1,var))
# maybe this range of variability is big in this context.
#Thus, we will use the correlation matrix
#For this, we must standardize our variables with scale() function:
all.stand <- as.data.frame(scale(app1))
sapply(all.stand,sd) #now, standard deviations are 1

#If we use prcomp() function, we indicate 'scale=TRUE' to use correlation matrix
pca <- prcomp(all.stand[,2:10],scale=T)
#it is just the same that: prcomp(iris[,1:4],scale=T) and prcomp(iris.stand)
#similar with princomp(): princomp(iris.stand, cor=T)
pca
summary(pca)
# Here, we figure out that the first 7 comps explain 0.96514 proportion already, so we can take them only for modeling.

pca$sdev
pca$rotation
pca=as.data.frame(pca)

#This gives us the standard deviation of each component, and the proportion of variance explained by each component.
#The standard deviation is stored in (see 'str(pca)'):

detach()


#Create 10 equally size folds
folds <- cut(seq(1,nrow(pca)),breaks=10,labels=FALSE)
library(gbm)
set.seed(1)
attach(app1)
pca1=pca[,1:7]
mse=c()
for (i in 1:10){
  testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- (data.matrix(pca1))[testIndexes, ]
    trainData <- (data.matrix(pca1))[-testIndexes, ]
  trainlabel=(data.matrix(PM2.5))[-testIndexes, ]
  testlabel=(data.matrix(PM2.5))[testIndexes, ]
  xgbml <- xgboost(data = trainData, label =trainlabel,max_depth=2, subsample=0.8, eta = 0.7, min_child_weight=8,objective = "reg:linear",nrounds=9)
  prediction <- predict(xgbml, newdata=testData)
  mse=mean((prediction-testlabel)^2)
}
mse
```







